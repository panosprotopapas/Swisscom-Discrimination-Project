{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: {'User-Agent': 'Opera/9.80 (X11; Linux i686; Ubuntu/14.10) Presto/2.12.388 Version/12.16'}\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import discrimination\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, Dense, Activation, Flatten, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras import regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import tqdm\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create lists containg all the *English* texts, clean them up, etc., and finally save them.\n",
    "\n",
    "Download from Mongo all the texts collected (tables are named diary.com, my-diary.org and everydaysexism) and create lists with them, one for each table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_diary=[]\n",
    "texts_mydiary = []\n",
    "texts_everydaysexism = []\n",
    "\n",
    "table = discrimination.mongo.collection(collection= \"diary.com\")\n",
    "for x in table.find():\n",
    "    texts_diary.append(x['text'])\n",
    "\n",
    "table = discrimination.mongo.collection(collection= \"my-diary.org\")\n",
    "for x in table.find():\n",
    "    texts_mydiary.append(x['text'])\n",
    "\n",
    "table = discrimination.mongo.collection(collection= \"everydaysexism\")\n",
    "for x in table.find():\n",
    "    texts_everydaysexism.append(x['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(texts_diary), \"texts collected from diary.com\")\n",
    "print(len(texts_mydiary), \"texts collected from my-diary.org\")\n",
    "print(len(texts_everydaysexism), \"texts collected from everydaysexism.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From each list of texts, keep only the ones in English. This process takes a long time... a timer is printed every 10,000 texts (by default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_diary = discrimination.texts.keep_english(texts_diary)\n",
    "texts_mydiary = discrimination.texts.keep_english(texts_mydiary)\n",
    "texts_everydaysexism = discrimination.texts.keep_english(texts_everydaysexism)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the texts from urls, html, hastags; then make them lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_diary = discrimination.texts.clean(texts_diary)\n",
    "texts_mydiary = discrimination.texts.clean(texts_mydiary)\n",
    "texts_everydaysexism = discrimination.texts.clean(texts_everydaysexism)\n",
    "\n",
    "texts_diary = discrimination.texts.lowercase(texts_diary)\n",
    "texts_mydiary = discrimination.texts.lowercase(texts_mydiary)\n",
    "texts_everydaysexism = discrimination.texts.lowercase(texts_everydaysexism)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the text-lists using Pickle to avoid redoing this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "# pickle.dump(texts_diary, open(\"pickles/texts_diary.p\", \"wb\"))\n",
    "# pickle.dump(texts_mydiary, open(\"pickles/texts_mydiary.p\", \"wb\"))\n",
    "# pickle.dump(texts_everydaysexism, open(\"pickles/texts_everydaysexism.p\", \"wb\"))\n",
    "\n",
    "# Load\n",
    "texts_diary = pickle.load(open(\"pickles/texts_diary.p\", \"rb\"))\n",
    "texts_mydiary = pickle.load(open(\"pickles/texts_mydiary.p\", \"rb\"))\n",
    "texts_everydaysexism = pickle.load(open(\"pickles/texts_everydaysexism.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(texts_diary), \"texts collected from diary.com after cleaning\")\n",
    "print(len(texts_mydiary), \"texts collected from my-diary.org after cleaning\")\n",
    "print(len(texts_everydaysexism), \"texts collected from everydaysexism.com after cleaning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split my-diary.org texts in sentences\n",
    "\n",
    "This step is done in order to split up the non-sexist texts from my-diary.org (since they are too long) in order to match the length of the sexist texts from everydaysexism. This greatly increases the number of texts obtained from my-diary.org (roughly by 10 times)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_diary = discrimination.texts.sentences_split(texts_diary)\n",
    "sentences_mydiary = discrimination.texts.sentences_split(texts_mydiary)\n",
    "sentences_everydaysexism = discrimination.texts.sentences_split(texts_everydaysexism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_texts = [sentences_everydaysexism, sentences_diary, sentences_mydiary]\n",
    "legend = [\"Everydaysexism.com\", \"Diary.com\", \"My-diary.org\"]\n",
    "discrimination.texts.sentences_plot(list_of_texts, (0,0.27), 30, [10,5], 90, legend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like texts from everydaysexism.com and diary.com have comparable lengths in terms of sentences. But my-diary.org texts seem way longer. Let's take a closer look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrimination.texts.sentences_plot(list_of_texts, (0,0.032), 50, [10,5], 90, legend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Texts from everydaysexism.com and diary.com have way less sentences than my-diary.org. Use a hard limit and split all texts of my-diary.org until they are up to 20 sentences long at most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "for sentences in sentences_mydiary:\n",
    "    division = round( len(sentences) / 20 )\n",
    "    for i in range(division + 1):\n",
    "        text = \"\".join(sentences[20*i : 20*(i+1)])\n",
    "        if len(text) >= 20:\n",
    "            texts.append(text)\n",
    "            \n",
    "sentences_mydiary2 = discrimination.texts.sentences_split(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_texts = [sentences_everydaysexism, sentences_diary, sentences_mydiary2]\n",
    "legend = [\"Everydaysexism.com\", \"Diary.com\", \"My-diary.org (first split)\"]\n",
    "discrimination.texts.sentences_plot(list_of_texts, (0,0.5), 30, [10,5], 90, legend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has created obviously a very large spike at 20 sentences (~45%) of the texts. What we can do is take a normal distribution with $\\mu=2$ and $\\sigma = 6$. Then we will discard non-positive values and round all other values to the closest integer. This draw will -almost- be a half-normal with $\\mu=2$ and $\\sigma = 6$. It will be able to reproduce, to some extent, the distribution of everydaysexism.com texts, where many texts have just one or two sentences, and will also have all 99.6% of texts having between 1 and $\\mu + 3\\times\\sigma = 20$ sentences. (It won't however look like a half-normal cause when splitting $x$, the number of sentnces in $x_1$ follows this distribution, but the number of sentences left in $x_2 = x -x_1$ does not).\n",
    "\n",
    "...well after a little tweaking, setting $\\mu=4$ and $\\sigma=7$ makes the distribution resemble the everyday sexism one. So I'll settle for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts.clear()\n",
    "\n",
    "for sentences in sentences_mydiary2:   \n",
    "    draw = -1\n",
    "    while draw <= 0:\n",
    "        draw = int(round(random.normalvariate(4,7),0))\n",
    "    \n",
    "    division = round( len(sentences) / draw )\n",
    "    \n",
    "    for i in range(division + 1):   \n",
    "        text = \"\".join(sentences[draw*i : draw*(i+1)])\n",
    "        if len(text) >= 20:\n",
    "            texts.append(text)\n",
    "\n",
    "sentences_mydiary3 = discrimination.texts.sentences_split(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_texts = [sentences_everydaysexism, sentences_diary, sentences_mydiary3]\n",
    "legend = [\"Everydaysexism.com\", \"Diary.com\", \"My-diary.org (second split)\"]\n",
    "discrimination.texts.sentences_plot(list_of_texts, (0,0.5), 30, [10,5], 90, legend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks good. Make \"texts_mydiary\" equal to the final \"texts\" variable and move on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_mydiary = texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"37K of English texts collected resulted in\", len(texts_mydiary), \"after splitting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize texts<sup>1</sup>, spell-check each word<sup>2</sup>, remove stop-words again, discard the empty resulting tokens and the ones with just one word. Save.\n",
    "\n",
    "<sup>1</sup> Tokens only contain letters, words with letters and numbers are discarded.  \n",
    "<sup>2</sup> If the word does not exist in the dictionary, then all existing words that are 1 letter \"change\" away are found. For example, from the words *cta*, *cet*, *ca*, *catt*, the word *cat* is respectively one neighbouring letter-swap, letter-change, letter addition and letter removal away. After all such words are found in the dictionary, the most frequent one is chosen to substitute the misspelled word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize texts and remove stop-words\n",
    "tokens_diary = discrimination.texts.tokenize(texts_diary)\n",
    "tokens_mydiary = discrimination.texts.tokenize(texts_mydiary)\n",
    "tokens_everydaysexism = discrimination.texts.tokenize(texts_everydaysexism)\n",
    "\n",
    "# Spell-check tokens. This actually takes some time (not too much) so there's a timer every 20.000 tokens checked.\n",
    "tokens_diary = discrimination.texts.spellcheck_tokens(tokens_diary)\n",
    "tokens_mydiary = discrimination.texts.spellcheck_tokens(tokens_mydiary)\n",
    "tokens_everydaysexism = discrimination.texts.spellcheck_tokens(tokens_everydaysexism)\n",
    "\n",
    "# Remove stop-words a second time, in case some stopwords where misspelled.\n",
    "tokens_diary = discrimination.texts.remove_stopwords(tokens_diary)\n",
    "tokens_mydiary = discrimination.texts.remove_stopwords(tokens_mydiary)\n",
    "tokens_everydaysexism = discrimination.texts.remove_stopwords(tokens_everydaysexism)\n",
    "\n",
    "# Before discarding some tokens, keep a copy of the original ones.\n",
    "tokens_diary_all = tokens_diary.copy()\n",
    "tokens_mydiary_all = tokens_mydiary.copy()\n",
    "tokens_everydaysexism_all = tokens_everydaysexism.copy()\n",
    "\n",
    "# Discard tokens with 0 or 1 words.\n",
    "for tokens in [tokens_diary, tokens_mydiary, tokens_everydaysexism]:\n",
    "    temp = tokens.copy()\n",
    "    tokens.clear()\n",
    "    for token in temp:\n",
    "        if len(token) > 1:\n",
    "            tokens.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "# pickle.dump(tokens_diary, open(\"pickles/tokens_diary.p\", \"wb\"))\n",
    "# pickle.dump(tokens_mydiary, open(\"pickles/tokens_mydiary.p\", \"wb\"))\n",
    "# pickle.dump(tokens_everydaysexism, open(\"pickles/tokens_everydaysexism.p\", \"wb\"))\n",
    "# pickle.dump(tokens_diary_all, open(\"pickles/tokens_diary_all.p\", \"wb\"))\n",
    "# pickle.dump(tokens_mydiary_all, open(\"pickles/tokens_mydiary_all.p\", \"wb\"))\n",
    "# pickle.dump(tokens_everydaysexism_all, open(\"pickles/tokens_everydaysexism_all.p\", \"wb\"))\n",
    "\n",
    "# Load\n",
    "tokens_diary = pickle.load(open(\"pickles/tokens_diary.p\", \"rb\"))\n",
    "tokens_mydiary = pickle.load(open(\"pickles/tokens_mydiary.p\", \"rb\"))\n",
    "tokens_everydaysexism = pickle.load(open(\"pickles/tokens_everydaysexism.p\", \"rb\"))\n",
    "tokens_diary_all = pickle.load(open(\"pickles/tokens_diary_all.p\", \"rb\"))\n",
    "tokens_mydiary_all = pickle.load(open(\"pickles/tokens_mydiary_all.p\", \"rb\"))\n",
    "tokens_everydaysexism_all = pickle.load(open(\"pickles/tokens_everydaysexism_all.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert tokens back to text (so Keras can be happy) and labels for these texts. Keep a record of original texts, tokens, tokens-to-texts, labels. Save.\n",
    "\n",
    "The record is needed so that later on we can check the prediction for tokens of interest (e.g. the borderline sexist ones) and easily access the original text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tokens back to text for Keras\n",
    "texts_keras = []\n",
    "for token in itertools.chain(tokens_diary, tokens_mydiary, tokens_everydaysexism):\n",
    "    text = \"\"\n",
    "    for word in token:\n",
    "        text += word + \" \"        \n",
    "    texts_keras.append(text)\n",
    "\n",
    "# Create labels\n",
    "labels_keras = np.zeros(len(texts_keras))\n",
    "labels_keras[-len(tokens_everydaysexism):] = 1\n",
    "\n",
    "# Record texts, tokens\n",
    "record = []\n",
    "for text, token in zip(texts_diary, tokens_diary_all):\n",
    "    record.append({\"text\": text, \"token\": token, \"truth\": 0})\n",
    "for text, token in zip(texts_mydiary, tokens_mydiary_all):\n",
    "    record.append({\"text\": text, \"token\": token, \"truth\": 0})\n",
    "for text, token in zip(texts_everydaysexism, tokens_everydaysexism_all):\n",
    "    record.append({\"text\": text, \"token\": token, \"truth\": 1})\n",
    "\n",
    "# Add the tokens-to-text in the record\n",
    "for entry in record:\n",
    "    token_to_text = \"\"\n",
    "    for word in entry[\"token\"]:\n",
    "        token_to_text += word + \" \"        \n",
    "    entry[\"token_to_text\"] = token_to_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "# pickle.dump(record, open(\"pickles/record.p\", \"wb\"))\n",
    "# pickle.dump(texts_keras, open(\"pickles/texts_keras.p\", \"wb\"))\n",
    "# pickle.dump(labels_keras, open(\"pickles/labels_keras.p\", \"wb\"))\n",
    "\n",
    "# Load\n",
    "record = pickle.load(open(\"pickles/record.p\", \"rb\"))\n",
    "texts_keras = pickle.load(open(\"pickles/texts_keras.p\", \"rb\"))\n",
    "labels_keras = pickle.load(open(\"pickles/labels_keras.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(labels_keras) - len(tokens_everydaysexism), \"non-sexist tokens and\", len(tokens_everydaysexism), \"sexist ones.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Neural Network in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\\Labels preparation and save. Parse the word embeddings, create the embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing - Sequencing\n",
    "tokenizer = Tokenizer(lower = False)\n",
    "tokenizer.fit_on_texts(texts_keras)\n",
    "sequences = tokenizer.texts_to_sequences(texts_keras)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Create and shuffle data and labels\n",
    "data = pad_sequences(sequences, maxlen=256)\n",
    "labels = np.asarray(labels_keras)\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "# Save data and labels\n",
    "pickle.dump(data, open(\"pickles/data.p\", \"wb\"))\n",
    "pickle.dump(labels, open(\"pickles/labels.p\", \"wb\"))\n",
    "\n",
    "# Split 80-20\n",
    "nb_validation_samples = int(0.2 * data.shape[0])\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the GloVe word embeddings and save\n",
    "glove_dir = \"glove/\"\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_dir, \"glove.42B.300d.txt\"))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the embedding matrix\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "# Delete the embeddings index as it's no longer needed.\n",
    "del embeddings_index\n",
    "# Create the embedding layer\n",
    "embedding_layer = Embedding(len(word_index) + 1, 300, input_length=256,\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network setup, compilation, results, testing. Save model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 256, 300)          26588700  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 76800)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 76800)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               19661056  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                4112      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 46,253,885\n",
      "Trainable params: 19,665,185\n",
      "Non-trainable params: 26,588,700\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation=\"relu\", kernel_regularizer = regularizers.l2(0.001)))\n",
    "model.add(Dense(16, activation=\"relu\", kernel_regularizer = regularizers.l2(0.001)))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 348965 samples, validate on 87241 samples\n",
      "Epoch 1/3\n",
      "348965/348965 [==============================] - 1178s 3ms/step - loss: 0.3975 - acc: 0.8993 - val_loss: 0.3453 - val_acc: 0.9252\n",
      "Epoch 2/3\n",
      "348965/348965 [==============================] - 3916s 11ms/step - loss: 0.3367 - acc: 0.9111 - val_loss: 0.2889 - val_acc: 0.9280\n",
      "Epoch 3/3\n",
      "348965/348965 [==============================] - 4325s 12ms/step - loss: 0.3167 - acc: 0.9146 - val_loss: 0.2808 - val_acc: 0.9281\n"
     ]
    }
   ],
   "source": [
    "# Compilation\n",
    "model.compile(optimizer = \"Adam\",\n",
    "              loss = \"binary_crossentropy\",\n",
    "              metrics = [\"acc\"])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs = 3,\n",
    "                    batch_size = 256,\n",
    "                    validation_data = (x_val, y_val))\n",
    "\n",
    "# Save model weights\n",
    "model.save_weights(\"pickles/model2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some graphs\n",
    "acc = history.history[\"acc\"]\n",
    "val_acc = history.history[\"val_acc\"]\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.figure(figsize = [12,6])\n",
    "\n",
    "# Accuracy graph\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, acc, \"bo\", label = \"Training acc\")\n",
    "plt.plot(epochs, val_acc, \"b\", label = \"Validation acc\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.ylim([0,1])\n",
    "plt.legend()\n",
    "# Loss graph\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, loss, \"bo\", label = \"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"b\", label = \"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.ylim([0,1])\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['rape', 'woman']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'63% sexist'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the network\n",
    "test = [\"\"]\n",
    "\n",
    "# Convert the test phrase to lowercase, tokenize, spellcheck, remove stopwords. \n",
    "test = discrimination.texts.lowercase(test)\n",
    "test = discrimination.texts.tokenize(test)\n",
    "test = discrimination.texts.spellcheck_tokens(test)\n",
    "test = discrimination.texts.remove_stopwords(test)\n",
    "\n",
    "# Convert the token back to text, sequence it, pad it, feed it into the model.\n",
    "text = \"\"\n",
    "for item in test:\n",
    "    for word in item:\n",
    "        text += word + \" \"   \n",
    "test_sequence = tokenizer.texts_to_sequences([text])\n",
    "print(test)\n",
    "x_test = pad_sequences(test_sequence, maxlen=256)\n",
    "model.load_weights(\"pickles/model2.h5\")\n",
    "# Make the output look pretty... because it deserves it.\n",
    "str(round(model.predict(x_test)[0,0]*100,0))[:-2] + \"% sexist\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Borderline results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all model predictions\n",
    "model.load_weights(\"pickles/model2.h5\")\n",
    "predictions = model.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the borderline\n",
    "borderline = [0.49, 0.51]\n",
    "\n",
    "# Find all tokens that are on the borderline\n",
    "borderline_tokens = []\n",
    "for i, prediction in enumerate(predictions):\n",
    "    if borderline[0] <= prediction[0] <= borderline[1]: \n",
    "        token = texts_keras[i]\n",
    "        sexist = labels_keras[i]\n",
    "        borderline_tokens.append((token, prediction[0], sexist))\n",
    "\n",
    "# Create a list with all the borderline texts\n",
    "borderline_texts = []\n",
    "for token in borderline_tokens:\n",
    "    if token[2]==1:\n",
    "        for entry in record:\n",
    "            if entry[\"token_to_text\"] == token[0]:\n",
    "                borderline_texts.append({\"text\": entry[\"text\"], \"prediction\": token[1], \"truth\": 1})\n",
    "    else:\n",
    "        for entry in record:\n",
    "            if entry[\"token_to_text\"] == token[0]:\n",
    "                borderline_texts.append({\"text\": entry[\"text\"], \"prediction\": token[1], \"truth\": 0})\n",
    "                \n",
    "# Make the list into a DataFrame\n",
    "df_borderline = pd.DataFrame(borderline_texts, columns = [\"text\", \"prediction\", \"truth\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "# pickle.dump(df_borderline, open(\"pickles/df_borderline.p\", \"wb\"))\n",
    "\n",
    "# Load\n",
    "df_borderline = pickle.load(open(\"pickles/df_borderline.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 0)\n",
    "df_borderline[df_borderline[\"truth\"]==1].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Word frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve the common vocabulary (sexist & non-sexist texts). Order it by frequency for each type of texts. Save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temp variables\n",
    "temp_list = []\n",
    "temp_sexist = []\n",
    "temp_nonsexist = []\n",
    "\n",
    "# Retrieve sexist vocabulary and convert tokens into sets\n",
    "sexist_vocabulary = set()\n",
    "for token in tokens_everydaysexism: \n",
    "    x = set(token)\n",
    "    sexist_vocabulary |= x\n",
    "    temp_sexist.append(x)\n",
    "\n",
    "# Retrieve non-sexist vocabulary and convert tokens into sets\n",
    "nonsexist_vocabulary = set()\n",
    "for token in itertools.chain(tokens_diary, tokens_mydiary):\n",
    "    x = set(token)\n",
    "    nonsexist_vocabulary |= x\n",
    "    temp_nonsexist.append(x)\n",
    "    \n",
    "# Retrieve common vocabulary\n",
    "common_vocabulary = sexist_vocabulary & nonsexist_vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I measure frequency by $\\frac{\\text{number of tokens word appears in}}{\\text{number of tokens}}$. This is done separately for each word in the sexist and non-sexist texts so that comparisons can be made. This takes some time (around 30 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency denominators\n",
    "denominator_sexist = len(temp_sexist)\n",
    "denominator_nonsexist = len(temp_nonsexist)\n",
    "\n",
    "# Frequency numerators\n",
    "for word in tqdm.tqdm(common_vocabulary):\n",
    "    timer += 1\n",
    "    numerator_sexist = 0\n",
    "    numerator_nonsexist = 0\n",
    "    \n",
    "    for token in temp_sexist:\n",
    "        if word in token:\n",
    "            token -= set(word)\n",
    "            numerator_sexist += 1\n",
    "            \n",
    "    for token in temp_nonsexist:\n",
    "        if word in token:\n",
    "            token -= set(word)\n",
    "            numerator_nonsexist += 1\n",
    "    \n",
    "    # Calculate word frequencies\n",
    "    pct_sexist = numerator_sexist / denominator_sexist\n",
    "    pct_nonsexist = numerator_nonsexist / denominator_nonsexist\n",
    "    \n",
    "    # Add info to the temp list\n",
    "    temp_list.append({'word': word, 'pct_sexist': pct_sexist, 'pct_nonsexist': pct_nonsexist})\n",
    "        \n",
    "# Make the list into a DataFrame\n",
    "df_word_rank = pd.DataFrame(temp_list, columns = [\"word\", \"pct_sexist\", \"pct_nonsexist\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "# pickle.dump(df_word_rank, open(\"pickles/df_word_rank.p\", \"wb\"))\n",
    "\n",
    "# Load\n",
    "df_word_rank = pickle.load(open(\"pickles/df_word_rank.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the word frequencies, create a dataframe so that the top-x most common words in sexist texts can be retrieved and compared with their non-sexist rank. Also do the opposite, i.e. top-x most common words in non-sexist texts can be retrieved and compared with their sexist rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show percentages as integers from 0 to 100.\n",
    "df_word_rank[\"pct_sexist\"] = round(df_word_rank[\"pct_sexist\"] * 100, 1)\n",
    "df_word_rank[\"pct_nonsexist\"] = round(df_word_rank[\"pct_nonsexist\"] * 100, 1)\n",
    "# Create NonSexist Rank\n",
    "df_word_rank = df_word_rank.sort_values(by=\"pct_nonsexist\", ascending = False)\n",
    "df_word_rank = df_word_rank.reset_index(drop=True)\n",
    "df_word_rank[\"nonsexist_rank\"] = 1 + df_word_rank.index\n",
    "# Create Sexist Rank\n",
    "df_word_rank = df_word_rank.sort_values(by=\"pct_sexist\", ascending = False)\n",
    "df_word_rank = df_word_rank.reset_index(drop=True)\n",
    "df_word_rank[\"sexist_rank\"] = 1 + df_word_rank.index\n",
    "# Reorder columns\n",
    "df_word_rank = df_word_rank[[\"word\", \"pct_sexist\", \"sexist_rank\", \"nonsexist_rank\", \"pct_nonsexist\"]]\n",
    "# Add Rank Difference, Absolute Rank Difference, and Rank Difference Squared columns\n",
    "df_word_rank[\"abs_rank_diff\"] = abs( df_word_rank[\"nonsexist_rank\"] - df_word_rank[\"sexist_rank\"] )\n",
    "df_word_rank[\"sq_rank_diff\"] = df_word_rank[\"abs_rank_diff\"] ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interested in Top-X, where...\n",
    "X = 20\n",
    "\n",
    "# Print Statistics\n",
    "print(\"Absolute Rank Difference (Sexist vs Non-Sexist) of top-100 words is \" + str(df_word_rank[\"abs_rank_diff\"][:X].mean()))\n",
    "print(\"Rank Difference Squared (Sexist vs Non-Sexist) of top-100 words is \" + str(df_word_rank[\"sq_rank_diff\"][:X].mean()))\n",
    "\n",
    "# Display the dataframe\n",
    "pd.set_option('display.max_rows', 50)\n",
    "df_word_rank.head(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order by NonSexist Rank\n",
    "df_word_rank = df_word_rank.sort_values(by=\"pct_nonsexist\", ascending = False)\n",
    "df_word_rank = df_word_rank.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Print Statistics\n",
    "print(\"Absolute Rank Difference (Non-Sexist vs Sexist) of top-100 words is \" + str(df_word_rank[\"abs_rank_diff\"][:X].mean()))\n",
    "print(\"Rank Difference Squared (Non-Sexist vs Sexist) of top-100 words is \" + str(df_word_rank[\"sq_rank_diff\"][:X].mean()))\n",
    "\n",
    "# Display the dataframe\n",
    "df_word_rank.head(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Confusion Matrix and distribution of scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a predicted labels list\n",
    "labels_predicted = []\n",
    "for prediction in predictions:\n",
    "    labels_predicted.append( round(prediction[0]) )\n",
    "# Calculate the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score\n",
    "CF = confusion_matrix(labels, labels_predicted)\n",
    "#\"Disentangle\" the matrix\n",
    "TN = round((CF[0,0] / sum(CF[0,:])) * 100, 1)\n",
    "FN = round((CF[0,1] / sum(CF[0,:])) * 100, 1)\n",
    "TP = round((CF[1,1] / sum(CF[1,:])) * 100, 1)\n",
    "FP = round((CF[1,0] / sum(CF[1,:])) * 100, 1)\n",
    "GTN = round((CF[0,0] / (sum(CF[0,:]) + sum(CF[1,:]))) * 100, 1)\n",
    "GFN = round((CF[0,1] / (sum(CF[0,:]) + sum(CF[1,:]))) * 100, 1)\n",
    "GTP = round((CF[1,1] / (sum(CF[0,:]) + sum(CF[1,:]))) * 100, 1)\n",
    "GFP = round((CF[1,0] / (sum(CF[0,:]) + sum(CF[1,:]))) * 100, 1)\n",
    "# Print the results\n",
    "print(\"True positives account for \"+str(TP)+\"% or \"+str(GTP)+\"% of the total (sexist texts labelled as sexist).\")\n",
    "print(\"True negatives account for \"+str(TN)+\"% or \"+str(GTN)+\"% of the total (non-sexist texts labelled as non-sexist).\")\n",
    "print(\"False positives account for \"+str(FP)+\"% or \"+str(GFP)+\"% of the total (sexist texts labelled as non-sexist).\")\n",
    "print(\"False negatives account for \"+str(FN)+\"% or \"+str(GFN)+\"% of the total (non-sexist texts labelled as sexist).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record all predictions (x) but also seperately for each class (x_sexist and x_nonsexist).\n",
    "x = []\n",
    "x_sexist = []\n",
    "x_nonsexist = []\n",
    "for i, p in enumerate(predictions):\n",
    "    x.append(p[0])\n",
    "    if labels[i] == 1:\n",
    "        x_sexist.append(p[0])\n",
    "    else:\n",
    "        x_nonsexist.append(p[0])\n",
    "\n",
    "# Transform predictions in the 0-100 range, and round them up to the nearast integer.        \n",
    "x_100 = []\n",
    "x_sexist_100 = []\n",
    "x_nonsexist_100 = []\n",
    "for item in x:\n",
    "    x_100.append(round(item*100))\n",
    "for item in x_sexist:\n",
    "    x_sexist_100.append(round(item*100))\n",
    "for item in x_nonsexist:\n",
    "    x_nonsexist_100.append(round(item*100))\n",
    "\n",
    "# Count the (rounded) predictions and transform to percentages.\n",
    "y = []\n",
    "y_sexist = []\n",
    "y_nonsexist = []\n",
    "for i in range(101):\n",
    "       y.append(100*x_100.count(i)/len(x_100))\n",
    "for i in range(101):\n",
    "       y_sexist.append(100*x_sexist_100.count(i)/len(x_sexist_100))\n",
    "for i in range(101):\n",
    "       y_nonsexist.append(100*x_nonsexist_100.count(i)/len(x_nonsexist_100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[14,4], dpi=110)\n",
    "plt.subplot(1,2,1)\n",
    "plt.ylim([0,20])\n",
    "plt.plot(range(101), y)\n",
    "plt.xlabel(\"Probability of sexist\")\n",
    "plt.ylabel(\"Percentage of texts\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.ylim([0,20])\n",
    "plt.plot(range(101), y_sexist)\n",
    "plt.plot(range(101), y_nonsexist)\n",
    "plt.legend([\"Sexist\", \"Non-Sexist\"])\n",
    "plt.xlabel(\"Probability of sexist\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zoom-in a little bit..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[14,4], dpi=110)\n",
    "plt.subplot(1,2,1)\n",
    "plt.ylim([0,3])\n",
    "plt.plot(range(101), y)\n",
    "plt.xlabel(\"Probability of sexist\")\n",
    "plt.ylabel(\"Percentage of texts\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.ylim([0,3])\n",
    "plt.plot(range(101), y_sexist)\n",
    "plt.plot(range(101), y_nonsexist)\n",
    "plt.legend([\"Sexist\", \"Non-Sexist\"])\n",
    "plt.xlabel(\"Probability of sexist\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
