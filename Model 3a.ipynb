{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: {'User-Agent': 'Mozilla/5.0 (compatible, MSIE 11, Windows NT 6.3; Trident/7.0; rv:11.0) like Gecko'}\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import discrimination\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, Dense, Activation, Flatten, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras import regularizers\n",
    "import itertools\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Get the *first* batch of data and make some statistics about them\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_diary = pickle.load(open(\"pickles/texts_diary.p\", \"rb\"))\n",
    "texts_mydiary = pickle.load(open(\"pickles/texts_mydiary.p\", \"rb\"))\n",
    "texts_everydaysexism = pickle.load(open(\"pickles/texts_everydaysexism.p\", \"rb\"))\n",
    "# split into sentences\n",
    "sentences_diary = discrimination.texts.sentences_split(texts_diary)\n",
    "sentences_mydiary = discrimination.texts.sentences_split(texts_mydiary)\n",
    "sentences_everydaysexism = discrimination.texts.sentences_split(texts_everydaysexism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of sentences per set\n",
    "nsd = []\n",
    "nsmd = []\n",
    "nses = []\n",
    "for s in sentences_diary:\n",
    "    nsd.append(len(s))\n",
    "for s in sentences_mydiary:\n",
    "    nsmd.append(len(s))\n",
    "for s in sentences_everydaysexism:\n",
    "    nses.append(len(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of words per sentence per set\n",
    "nwd = []\n",
    "nwmd = []\n",
    "nwes = []\n",
    "for s in sentences_diary:\n",
    "    item = []\n",
    "    for i in s:\n",
    "        n = len(re.findall('\\w+', i)) \n",
    "        item.append(n)\n",
    "    nwd.append(item)\n",
    "for s in sentences_mydiary:\n",
    "    item = []\n",
    "    for i in s:\n",
    "        n = len(re.findall('\\w+', i)) \n",
    "        item.append(n)\n",
    "    nwmd.append(item)\n",
    "for s in sentences_everydaysexism:\n",
    "    item = []\n",
    "    for i in s:\n",
    "        n = len(re.findall('\\w+', i)) \n",
    "        item.append(n)\n",
    "    nwes.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average sentences\n",
    "print(\"Average number of sentences in diary is\", sum(nsd)/len(nsd))\n",
    "print(\"Average number of sentences in my-diary is\", sum(nsmd)/len(nsmd))\n",
    "print(\"Average number of sentences in everydaysexism is\", sum(nses)/len(nses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average number of words-per-sentence (each text carries an equal weight)\n",
    "wpsewd = []\n",
    "wpsewmd = []\n",
    "wpsewes = []\n",
    "for i, n in enumerate(nwd):\n",
    "    if nsd[i] == 0:\n",
    "        wpsewd.append(0)\n",
    "    else:\n",
    "        wpsewd.append(sum(n)/nsd[i])\n",
    "for i, n in enumerate(nwmd):\n",
    "    if nsmd[i] == 0:\n",
    "        wpsewmd.append(0)\n",
    "    else:\n",
    "        wpsewmd.append(sum(n)/nsmd[i])\n",
    "for i, n in enumerate(nwes):\n",
    "    if nses[i] == 0:\n",
    "        wpsewes.append(0)\n",
    "    else:\n",
    "        wpsewes.append(sum(n)/nses[i])\n",
    "        \n",
    "print(\"Av. number of words-per-sentence (each text has an equal weight) in diary is\", round(sum(wpsewd)/len(wpsewd),2))\n",
    "print(\"Av. number of words-per-sentence (each text has an equal weight) in my diary is\", round(sum(wpsewmd)/len(wpsewmd),2))\n",
    "print(\"Av. number of words-per-sentence (each text has an equal weight) in everyday sexism is\", round(sum(wpsewes)/len(wpsewes), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average number of words-per-sentence (each sentence carries an equal weight)\n",
    "twd = []\n",
    "twmd = []\n",
    "twes = []\n",
    "for n in nwd:\n",
    "    twd.append(sum(n))\n",
    "for n in nwmd:\n",
    "    twmd.append(sum(n))\n",
    "for n in nwes:\n",
    "    twes.append(sum(n))\n",
    "        \n",
    "print(\"Av. number of words-per-sentence (each sentence has an equal weight) in diary is\", round(sum(twd)/sum(nsd),2))\n",
    "print(\"Av. number of words-per-sentence (each sentence has an equal weight) in my diary is\", round(sum(twmd)/sum(nsmd),2))\n",
    "print(\"Av. number of words-per-sentence (each sentence has an equal weight) in everyday sexism is\", round(sum(twes)/sum(nses),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Split my-diary texts in sentences and tokenize\n",
    "---\n",
    "\n",
    "This step is (re)done in order to split up the non-sexist texts from my-diary.org (since they are too long) in order to match the length of the sexist texts from everydaysexism. This greatly increases the number of texts obtained from my-diary.org. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload and resplit into sentences\n",
    "texts_diary = pickle.load(open(\"pickles/texts_diary.p\", \"rb\"))\n",
    "texts_mydiary = pickle.load(open(\"pickles/texts_mydiary.p\", \"rb\"))\n",
    "texts_everydaysexism = pickle.load(open(\"pickles/texts_everydaysexism.p\", \"rb\"))\n",
    "\n",
    "sentences_diary = discrimination.texts.sentences_split(texts_diary)\n",
    "sentences_mydiary = discrimination.texts.sentences_split(texts_mydiary)\n",
    "sentences_everydaysexism = discrimination.texts.sentences_split(texts_everydaysexism)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Texts from everydaysexism.com and diary.com have way less sentences than my-diary.org. Use a hard limit and split all texts of my-diary.org until they are up to 20 sentences long at most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "for sentences in sentences_mydiary:\n",
    "    division = round( len(sentences) / 20 )\n",
    "    for i in range(division + 1):\n",
    "        text = \"\".join(sentences[20*i : 20*(i+1)])\n",
    "        if len(text) >= 20:\n",
    "            texts.append(text)\n",
    "            \n",
    "sentences_mydiary2 = discrimination.texts.sentences_split(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has created obviously a very large spike at 20 sentences (~45%) of the texts. What we are interested to do is to have a similar average token length between the sexist and the non-sexist tokens. Will need to draw values from a normal distribution, discard negative values, and split the texts collected from my-diary according to this number until a similar average is \"generated\". It will take some fiddling around..\n",
    "\n",
    "..well it is slightly more complex than expected but it does an ok job in mimicking the distribution of words-per-token in the sexist texts **assuming** one disregards the spike observed in said text for tokens containing around 10 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the texts according to a normal with:\n",
    "mean = 7\n",
    "st_dev = 8\n",
    "\n",
    "texts.clear()\n",
    "\n",
    "for sentences in sentences_mydiary2:   \n",
    "    draw = -1\n",
    "    while draw <= 0:\n",
    "        draw = int(round(random.normalvariate(mean,st_dev),0))\n",
    "        while draw > 10 and random.random() < 0.3:\n",
    "            draw = int(round(random.normalvariate(mean,st_dev),0))\n",
    "        while draw == 1 and random.random() < 0.9:\n",
    "            draw = int(round(random.normalvariate(mean,st_dev),0))\n",
    "        while draw >=20 and random.random() < 0.75:\n",
    "            draw = int(round(random.normalvariate(mean,st_dev),0))\n",
    "        while 2 <= draw <= 4 and random.random() < 0.2:\n",
    "            draw = int(round(random.normalvariate(mean,st_dev),0))\n",
    "    \n",
    "    division = round( len(sentences) / draw )\n",
    "    \n",
    "    for i in range(division + 1):   \n",
    "        text = \"\".join(sentences[draw*i : draw*(i+1)])\n",
    "        if len(text) >= 20:\n",
    "            texts.append(text)\n",
    "texts_mydiary = texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize texts and remove stop-words\n",
    "tokens_diary = discrimination.texts.tokenize(texts_diary)\n",
    "tokens_mydiary = discrimination.texts.tokenize(texts_mydiary)\n",
    "tokens_everydaysexism = discrimination.texts.tokenize(texts_everydaysexism)\n",
    "\n",
    "# Spell-check tokens. This actually takes some time (not too much) so there's a timer every 20.000 tokens checked.\n",
    "tokens_diary = discrimination.texts.spellcheck_tokens(tokens_diary)\n",
    "tokens_mydiary = discrimination.texts.spellcheck_tokens(tokens_mydiary)\n",
    "tokens_everydaysexism = discrimination.texts.spellcheck_tokens(tokens_everydaysexism)\n",
    "\n",
    "# Remove stop-words a second time, in case some stopwords where misspelled.\n",
    "tokens_diary = discrimination.texts.remove_stopwords(tokens_diary)\n",
    "tokens_mydiary = discrimination.texts.remove_stopwords(tokens_mydiary)\n",
    "tokens_everydaysexism = discrimination.texts.remove_stopwords(tokens_everydaysexism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nw_diary = 0\n",
    "nw_mydiary = 0\n",
    "nw_everydaysexism = 0\n",
    "for token in tokens_diary:\n",
    "    nw_diary += len(token)\n",
    "for token in tokens_mydiary:\n",
    "    nw_mydiary += len(token)\n",
    "for token in tokens_everydaysexism:\n",
    "    nw_everydaysexism += len(token)\n",
    "    \n",
    "isit33 = (nw_diary + nw_mydiary ) / (len(tokens_diary)+len(tokens_mydiary))\n",
    "    \n",
    "print(\"Av. number of words-per-token in non-sexist is\", round(isit33,1))\n",
    "print(\"Av. number of words-per-token in sexist is\", round(nw_everydaysexism/len(tokens_everydaysexism),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_mydiary.extend(tokens_diary)\n",
    "list_of_texts = [tokens_everydaysexism, tokens_mydiary]\n",
    "legend = [\"Sexist\", \"Non-Sexist\"]\n",
    "discrimination.texts.sentences_plot(list_of_texts, (0,0.06), 100, [10,5], 90, legend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"37K of English texts collected resulted in\", len(texts_mydiary), \"after splitting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_mydiary.extend(texts_diary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "pickle.dump(tokens_everydaysexism, open(\"pickles2/tkn_sexist.p\", \"wb\"))\n",
    "pickle.dump(tokens_mydiary, open(\"pickles2/tkn_notsexist.p\", \"wb\"))\n",
    "pickle.dump(texts_mydiary, open(\"pickles2/txts_sexist.p\", \"wb\"))\n",
    "pickle.dump(texts_everydaysexism, open(\"pickles2/txts_notsexist.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Load and clean new texts\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txts_mgtow = []\n",
    "table = discrimination.mongo.collection(collection=\"mgtow\")\n",
    "for x in table.find():\n",
    "    txts_mgtow.append(x[\"text\"]) \n",
    "    \n",
    "txts_breitbart = []   \n",
    "table = discrimination.mongo.collection(collection=\"breitbart\")\n",
    "for x in table.find():\n",
    "    txts_breitbart.append(x[\"text\"])\n",
    "\n",
    "txts_9gag = []\n",
    "table = discrimination.mongo.collection(collection=\"9gag\")\n",
    "for x in table.find():\n",
    "    txts_9gag.append(x[\"text\"]) \n",
    "\n",
    "txts_misc = []\n",
    "table = discrimination.mongo.collection(collection=\"misc_texts\")\n",
    "for x in table.find():\n",
    "    txts_misc.append(x[\"text\"]) \n",
    "    \n",
    "txts_youtube = []   \n",
    "table = discrimination.mongo.collection(collection=\"youtube\")\n",
    "for x in table.find():\n",
    "    txts_youtube.append(x[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep English, clean, save\n",
    "txts_mgtow = discrimination.texts.keep_english(txts_mgtow, notify = 25000)\n",
    "txts_mgtow = discrimination.texts.clean(txts_mgtow)\n",
    "txts_mgtow = discrimination.texts.lowercase(txts_mgtow)\n",
    "pickle.dump(txts_mgtow, open(\"pickles2/txts_mgtow.p\", \"wb\"))\n",
    "\n",
    "txts_breitbart = discrimination.texts.keep_english(txts_breitbart, notify = 25000)\n",
    "txts_breitbart = discrimination.texts.clean(txts_breitbart)\n",
    "txts_mgtow = discrimination.texts.lowercase(txts_breitbart)\n",
    "pickle.dump(txts_breitbart, open(\"pickles2/txts_breitbart.p\", \"wb\"))\n",
    "\n",
    "txts_9gag = discrimination.texts.keep_english(txts_9gag, notify = 25000)\n",
    "txts_9gag = discrimination.texts.clean(txts_9gag)\n",
    "txts_mgtow = discrimination.texts.lowercase(txts_9gag)\n",
    "pickle.dump(txts_9gag, open(\"pickles2/txts_9gag.p\", \"wb\"))\n",
    "\n",
    "txts_misc = discrimination.texts.keep_english(txts_misc, notify = 25000)\n",
    "txts_misc = discrimination.texts.clean(txts_misc)\n",
    "txts_mgtow = discrimination.texts.lowercase(txts_misc)\n",
    "pickle.dump(txts_misc, open(\"pickles2/txts_misc.p\", \"wb\"))\n",
    "\n",
    "txts_youtube = discrimination.texts.keep_english(txts_youtube, notify = 25000)\n",
    "txts_youtube = discrimination.texts.clean(txts_youtube)\n",
    "txts_mgtow = discrimination.texts.lowercase(txts_youtube)\n",
    "pickle.dump(txts_youtube, open(\"pickles2/txts_youtube.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Tokenize new texts\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "txts_mgtow = pickle.load(open(\"pickles2/txts_mgtow.p\", \"rb\"))\n",
    "txts_breitbart = pickle.load(open(\"pickles2/txts_breitbart.p\", \"rb\"))\n",
    "txts_9gag = pickle.load(open(\"pickles2/txts_9gag.p\", \"rb\"))\n",
    "txts_misc = pickle.load(open(\"pickles2/txts_misc.p\", \"rb\"))\n",
    "txts_youtube = pickle.load(open(\"pickles2/txts_youtube.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize, remove stopwords, spellcheck\n",
    "tkn_mgtow = discrimination.texts.tokenize(txts_mgtow)\n",
    "tkn_mgtow = discrimination.texts.spellcheck_tokens(tkn_mgtow)\n",
    "tkn_mgtow = discrimination.texts.remove_stopwords(tkn_mgtow)\n",
    "\n",
    "tkn_breitbart = discrimination.texts.tokenize(txts_breitbart)\n",
    "tkn_breitbart = discrimination.texts.spellcheck_tokens(tkn_breitbart)\n",
    "tkn_breitbart = discrimination.texts.remove_stopwords(tkn_breitbart)\n",
    "\n",
    "tkn_9gag = discrimination.texts.tokenize(txts_9gag)\n",
    "tkn_9gag = discrimination.texts.spellcheck_tokens(tkn_9gag)\n",
    "tkn_9gag = discrimination.texts.remove_stopwords(tkn_9gag)\n",
    "\n",
    "tkn_misc = discrimination.texts.tokenize(txts_misc)\n",
    "tkn_misc = discrimination.texts.spellcheck_tokens(tkn_misc)\n",
    "tkn_misc = discrimination.texts.remove_stopwords(tkn_misc)\n",
    "\n",
    "tkn_youtube = discrimination.texts.tokenize(txts_youtube)\n",
    "tkn_youtube = discrimination.texts.spellcheck_tokens(tkn_youtube)\n",
    "tkn_youtube = discrimination.texts.remove_stopwords(tkn_youtube)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "pickle.dump(tkn_mgtow, open(\"pickles2/tkn_mgtow.p\", \"wb\"))\n",
    "pickle.dump(tkn_breitbart, open(\"pickles2/tkn_breitbart.p\", \"wb\"))\n",
    "pickle.dump(tkn_9gag, open(\"pickles2/tkn_9gag.p\", \"wb\"))\n",
    "pickle.dump(tkn_misc, open(\"pickles2/tkn_misc.p\", \"wb\"))\n",
    "pickle.dump(tkn_youtube, open(\"pickles2/tkn_youtube.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Convert all (new and old) tokens back to text. Label the old texts. Save.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load new tokens\n",
    "tkn_mgtow = pickle.load(open(\"pickles2/tkn_mgtow.p\", \"rb\"))\n",
    "tkn_breitbart = pickle.load(open(\"pickles2/tkn_breitbart.p\", \"rb\"))\n",
    "tkn_9gag = pickle.load(open(\"pickles2/tkn_9gag.p\", \"rb\"))\n",
    "tkn_misc = pickle.load(open(\"pickles2/tkn_misc.p\", \"rb\"))\n",
    "tkn_youtube = pickle.load(open(\"pickles2/tkn_youtube.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labelled tokens\n",
    "tkn_sexist = pickle.load(open(\"pickles2/tkn_sexist.p\", \"rb\"))\n",
    "tkn_notsexist = pickle.load(open(\"pickles2/tkn_notsexist.p\", \"rb\"))\n",
    "\n",
    "# Load labelled texts\n",
    "txts_sexist = pickle.load(open(\"pickles2/txts_sexist.p\", \"rb\"))\n",
    "txts_notsexist = pickle.load(open(\"pickles2/txts_notsexist.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tokens back to text for Keras\n",
    "txts_keras_label = []\n",
    "for tkn in itertools.chain(tkn_sexist, tkn_notsexist):\n",
    "    txt = \"\"\n",
    "    for wd in tkn:\n",
    "        txt += wd + \" \"        \n",
    "    txts_keras_label.append(txt)\n",
    "    \n",
    "txts_keras_nolabel = []\n",
    "for tkn in itertools.chain(tkn_mgtow, tkn_breitbart, tkn_9gag, tkn_misc, tkn_youtube):\n",
    "    txt = \"\"\n",
    "    for wd in tkn:\n",
    "        txt += wd + \" \"        \n",
    "    txts_keras_nolabel.append(txt)\n",
    "    \n",
    "# Create labels\n",
    "label_keras = np.zeros(len(txts_keras_label))\n",
    "label_keras[:len(tkn_sexist)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a single text list\n",
    "txts_new = []\n",
    "for txt in txts_mgtow:\n",
    "    txts_new.append((txt, \"mgtow\"))\n",
    "for txt in txts_breitbart:\n",
    "    txts_new.append((txt, \"breitbart\"))\n",
    "for txt in txts_9gag:\n",
    "    txts_new.append((txt, \"9gag\"))\n",
    "for txt in txts_misc:\n",
    "    txts_new.append((txt, \"misc\"))\n",
    "for txt in txts_youtube:\n",
    "    txts_new.append((txt, \"youtube\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "pickle.dump(txts_keras_label, open(\"pickles2/txts_keras_label.p\", \"wb\"))\n",
    "pickle.dump(txts_keras_nolabel, open(\"pickles2/txts_keras_nolabel.p\", \"wb\"))\n",
    "pickle.dump(label_keras, open(\"pickles2/label_keras.p\", \"wb\"))\n",
    "pickle.dump(txts_new, open(\"pickles2/txts_new.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### NN preparation\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "txts_keras_label = pickle.load(open(\"pickles2/txts_keras_label.p\", \"rb\"))\n",
    "txts_keras_nolabel = pickle.load(open(\"pickles2/txts_keras_nolabel.p\", \"rb\"))\n",
    "label_keras = pickle.load(open(\"pickles2/label_keras.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txts4fit = txts_keras_label.copy()\n",
    "txts4fit.extend(txts_keras_nolabel) \n",
    "# Tokenizing - Sequencing\n",
    "tokenizer = Tokenizer(lower = False)\n",
    "tokenizer.fit_on_texts(txts4fit)\n",
    "sequences = tokenizer.texts_to_sequences(txts_keras_label)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Create and shuffle data and labels\n",
    "data = pad_sequences(sequences, maxlen=256)\n",
    "labels = np.asarray(label_keras)\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "# Split 80-20\n",
    "nb_validation_samples = int(0.2 * data.shape[0])\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the GloVe word embeddings\n",
    "glove_dir = \"glove/\"\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_dir, \"glove.42B.300d.txt\"))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the embedding matrix\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "# Delete the embeddings index as it's no longer needed.\n",
    "del embeddings_index\n",
    "# Create the embedding layer\n",
    "embedding_layer = Embedding(len(word_index) + 1, 300, input_length=256,\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### NN setup and compilation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation=\"relu\", kernel_regularizer = regularizers.l2(0.001)))\n",
    "model.add(Dense(16, activation=\"relu\", kernel_regularizer = regularizers.l2(0.001)))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilation\n",
    "model.compile(optimizer = \"Adam\",\n",
    "              loss = \"binary_crossentropy\",\n",
    "              metrics = [\"acc\"])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs = 3,\n",
    "                    batch_size = 256,\n",
    "                    validation_data = (x_val, y_val))\n",
    "\n",
    "# Save model weights\n",
    "model.save_weights(\"pickles2/model3.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Confusion Matrix\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "txts_keras_nolabel = pickle.load(open(\"pickles2/txts_keras_nolabel.p\", \"rb\"))\n",
    "txts_keras_label = pickle.load(open(\"pickles2/txts_keras_label.p\", \"rb\"))\n",
    "label_keras = pickle.load(open(\"pickles2/label_keras.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Tokenizer, sequence, etcρίξει την αγνωστικιστική εκκλ\n",
    "txts4fit = txts_keras_label.copy()\n",
    "txts4fit.extend(txts_keras_nolabel) \n",
    "tokenizer = Tokenizer(lower = False)\n",
    "tokenizer.fit_on_texts(txts4fit)\n",
    "del txts_keras_nolabel\n",
    "sequences = tokenizer.texts_to_sequences(txts_keras_label)\n",
    "word_index = tokenizer.word_index\n",
    "data = pad_sequences(sequences, maxlen=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the GloVe word embeddings\n",
    "glove_dir = \"glove/\"\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_dir, \"glove.42B.300d.txt\"))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding layer\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "del embeddings_index\n",
    "embedding_layer = Embedding(len(word_index) + 1, 300, input_length=256,\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and weights\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation=\"relu\", kernel_regularizer = regularizers.l2(0.001)))\n",
    "model.add(Dense(16, activation=\"relu\", kernel_regularizer = regularizers.l2(0.001)))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.summary()\n",
    "model.load_weights(\"pickles2/model3.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "predictions_CF = model.predict(data)\n",
    "# Save\n",
    "pickle.dump(predictions_CF, open(\"pickles2/predictions_CF.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positives account for 83.9% or 22.8% of the total (sexist texts labelled as sexist).\n",
      "True negatives account for 97.0% or 70.7% of the total (non-sexist texts labelled as non-sexist).\n",
      "False positives account for 16.1% or 4.4% of the total (sexist texts labelled as non-sexist).\n",
      "False negatives account for 3.0% or 2.2% of the total (non-sexist texts labelled as sexist).\n"
     ]
    }
   ],
   "source": [
    "# Load labels and predictions\n",
    "label_keras = pickle.load(open(\"pickles2/label_keras.p\", \"rb\"))\n",
    "predictions_CF = pickle.load(open(\"pickles2/predictions_CF.p\", \"rb\"))\n",
    "\n",
    "# Create a predicted labels list\n",
    "labels_predicted = []\n",
    "for prediction in predictions_CF:\n",
    "    labels_predicted.append( round(prediction[0]) )\n",
    "# Calculate the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score\n",
    "CF = confusion_matrix(label_keras, labels_predicted)\n",
    "#\"Disentangle\" the matrix\n",
    "TN = round((CF[0,0] / sum(CF[0,:])) * 100, 1)\n",
    "FN = round((CF[0,1] / sum(CF[0,:])) * 100, 1)\n",
    "TP = round((CF[1,1] / sum(CF[1,:])) * 100, 1)\n",
    "FP = round((CF[1,0] / sum(CF[1,:])) * 100, 1)\n",
    "GTN = round((CF[0,0] / (sum(CF[0,:]) + sum(CF[1,:]))) * 100, 1)\n",
    "GFN = round((CF[0,1] / (sum(CF[0,:]) + sum(CF[1,:]))) * 100, 1)\n",
    "GTP = round((CF[1,1] / (sum(CF[0,:]) + sum(CF[1,:]))) * 100, 1)\n",
    "GFP = round((CF[1,0] / (sum(CF[0,:]) + sum(CF[1,:]))) * 100, 1)\n",
    "# Print the results\n",
    "print(\"True positives account for \"+str(TP)+\"% or \"+str(GTP)+\"% of the total (sexist texts labelled as sexist).\")\n",
    "print(\"True negatives account for \"+str(TN)+\"% or \"+str(GTN)+\"% of the total (non-sexist texts labelled as non-sexist).\")\n",
    "print(\"False positives account for \"+str(FP)+\"% or \"+str(GFP)+\"% of the total (sexist texts labelled as non-sexist).\")\n",
    "print(\"False negatives account for \"+str(FN)+\"% or \"+str(GFN)+\"% of the total (non-sexist texts labelled as sexist).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Check new texts for sexism\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "txts_keras_nolabel = pickle.load(open(\"pickles2/txts_keras_nolabel.p\", \"rb\"))\n",
    "txts_keras_label = pickle.load(open(\"pickles2/txts_keras_label.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Tokenizer, sequence, etc\n",
    "txts4fit = txts_keras_label.copy()\n",
    "txts4fit.extend(txts_keras_nolabel) \n",
    "tokenizer = Tokenizer(lower = False)\n",
    "tokenizer.fit_on_texts(txts4fit)\n",
    "del txts_keras_label\n",
    "sequences = tokenizer.texts_to_sequences(txts_keras_nolabel)\n",
    "word_index = tokenizer.word_index\n",
    "data = pad_sequences(sequences, maxlen=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the GloVe word embeddings\n",
    "glove_dir = \"glove/\"\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_dir, \"glove.42B.300d.txt\"))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding layer\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "del embeddings_index\n",
    "embedding_layer = Embedding(len(word_index) + 1, 300, input_length=256,\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 256, 300)          81110400  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 76800)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 76800)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               19661056  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                4112      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 100,775,585\n",
      "Trainable params: 19,665,185\n",
      "Non-trainable params: 81,110,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Load model and weights\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation=\"relu\", kernel_regularizer = regularizers.l2(0.001)))\n",
    "model.add(Dense(16, activation=\"relu\", kernel_regularizer = regularizers.l2(0.001)))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.summary()\n",
    "model.load_weights(\"pickles2/model3.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "predictions = model.predict(data)\n",
    "# Save\n",
    "pickle.dump(predictions, open(\"pickles2/predictions.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Texts predicted to be 90%, or more, sexist are kept as sexist. Texts predicted to be 5%, or less, sexist are kept as non-sexist.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "predictions = pickle.load(open(\"pickles2/predictions.p\", \"rb\"))\n",
    "txts_new = pickle.load(open(\"pickles2/txts_new.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_cut = 0.95\n",
    "low_cut = 0.1\n",
    "\n",
    "sexist_new = []\n",
    "nonsexist_new = []\n",
    "for p, t in zip(predictions, txts_new):\n",
    "    if p > high_cut:\n",
    "        sexist_new.append(t)\n",
    "    elif p < low_cut:\n",
    "        nonsexist_new.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 72318 new sexist texts and 408254 new non-sexist texts.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are\", len(sexist_new), \"new sexist texts and\", len(nonsexist_new), \"new non-sexist texts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "texts = [] \n",
    "for item in sexist_new:\n",
    "    if not re.search('woman|women|man|men', item[0]):\n",
    "        texts.append(item[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "random.sample(texts, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "pickle.dump(sexist_new, open(\"pickles2/sexist_new.p\", \"wb\"))\n",
    "pickle.dump(nonsexist_new, open(\"pickles2/nonsexist_new.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'27% sexist'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the network\n",
    "test = ['''She looks like a slut''']\n",
    "\n",
    "# Convert the test phrase to lowercase, tokenize, spellcheck, remove stopwords. \n",
    "test = discrimination.texts.lowercase(test)\n",
    "test = discrimination.texts.tokenize(test)\n",
    "test = discrimination.texts.spellcheck_tokens(test)\n",
    "test = discrimination.texts.remove_stopwords(test)\n",
    "\n",
    "# Convert the token back to text, sequence it, pad it, feed it into the model.\n",
    "text = \"\"\n",
    "for item in test:\n",
    "    for word in item:\n",
    "        text += word + \" \"   \n",
    "test_sequence = tokenizer.texts_to_sequences([text])\n",
    "\n",
    "x_test = pad_sequences(test_sequence, maxlen=256)\n",
    "model.load_weights(\"pickles2/model3.h5\")\n",
    "# Make the output look pretty... because it deserves it.\n",
    "str(round(model.predict(x_test)[0,0]*100,0))[:-2] + \"% sexist\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
